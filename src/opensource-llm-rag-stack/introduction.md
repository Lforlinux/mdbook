# Opensource-LLM-RAG-Stack

A production-ready, containerized RAG (Retrieval-Augmented Generation) stack with comprehensive monitoring, observability, and enterprise-grade DevOps practices.

## Overview

The Opensource-LLM-RAG-Stack is a complete, self-contained RAG system that demonstrates enterprise-grade AI infrastructure practices. It combines vector databases, LLM inference, web interfaces, and comprehensive monitoring in a single Docker Compose setup.

![OpenSource RAG LLM Stack Architecture](../images/RAG-LLM-stack.png)

## Key Features

- **Containerized Microservices**: Docker Compose orchestration with complete service isolation
- **Vector Database**: Chroma for semantic search and embeddings storage
- **LLM Integration**: Containerized Ollama for reproducible LLM inference with Open WebUI interface
- **Data Persistence**: PostgreSQL with optimized schema for chat history and RAG documents
- **Observability**: Prometheus metrics collection with Grafana dashboards
- **Monitoring**: Real-time service health monitoring and performance metrics
- **Security**: Network isolation, environment-based configuration, and data encryption

## Project Highlights

This project demonstrates:
- Modern AI/ML infrastructure patterns
- Containerized microservices architecture
- Vector database integration
- Comprehensive observability
- Production-ready RAG implementation
- Enterprise DevOps practices

